<!DOCTYPE html>
<html lang="tr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Linux + NVIDIA Fine-Tuning Araclari</title>
    <link rel="stylesheet" href="../css/style.css">
</head>
<body>
    <header>
        <div class="container">
            <h1>Linux + NVIDIA Fine-Tuning Araclari</h1>
            <p>Profesyonel AI Gelistirme Icin En Kapsamli Arac Seti</p>
        </div>
    </header>

    <nav>
        <div class="container">
            <ul>
                <li><a href="../index.html">Ana Sayfa</a></li>
                <li><a href="index.html">Linux + NVIDIA</a></li>
                <li><a href="kurulum.html">Kurulum</a></li>
                <li><a href="araclar.html" class="active">Araclar</a></li>
                <li><a href="fine-tuning-rehberi.html">Fine-Tuning</a></li>
            </ul>
        </div>
    </nav>

    <div class="container">
        <div class="breadcrumb">
            <a href="../index.html">Ana Sayfa</a> &gt;
            <a href="index.html">Linux + NVIDIA</a> &gt; Araclar
        </div>
    </div>

    <main>
        <div class="container">
            <!-- Genel Bakis -->
            <section>
                <h2>Arac Karsilastirmasi</h2>

                <div class="alert alert-success">
                    <strong>Linux + NVIDIA Avantaji:</strong> Tum fine-tuning araclari bu platformda
                    tam desteklidir. En genis seceneklere ve en iyi performansa bu platformda ulasirsiniz.
                </div>

                <div class="comparison-table">
                    <table>
                        <thead>
                            <tr>
                                <th>Arac</th>
                                <th>Zorluk</th>
                                <th>Hiz</th>
                                <th>Bellek</th>
                                <th>En Iyi Icin</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Unsloth</strong></td>
                                <td>Kolay</td>
                                <td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
                                <td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
                                <td>Hiz ve verimlilik</td>
                            </tr>
                            <tr>
                                <td><strong>Axolotl</strong></td>
                                <td>Orta</td>
                                <td>‚≠ê‚≠ê‚≠ê‚≠ê</td>
                                <td>‚≠ê‚≠ê‚≠ê‚≠ê</td>
                                <td>Esneklik, coklu GPU</td>
                            </tr>
                            <tr>
                                <td><strong>Llama Factory</strong></td>
                                <td>Cok Kolay</td>
                                <td>‚≠ê‚≠ê‚≠ê</td>
                                <td>‚≠ê‚≠ê‚≠ê‚≠ê</td>
                                <td>Yeni baslayanlar</td>
                            </tr>
                            <tr>
                                <td><strong>Torchtune</strong></td>
                                <td>Orta</td>
                                <td>‚≠ê‚≠ê‚≠ê‚≠ê</td>
                                <td>‚≠ê‚≠ê‚≠ê‚≠ê</td>
                                <td>PyTorch kullanicilari</td>
                            </tr>
                            <tr>
                                <td><strong>DeepSpeed</strong></td>
                                <td>Zor</td>
                                <td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
                                <td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
                                <td>Buyuk modeller, coklu GPU</td>
                            </tr>
                            <tr>
                                <td><strong>Megatron-LM</strong></td>
                                <td>Cok Zor</td>
                                <td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
                                <td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
                                <td>Kurumsal, cok buyuk modeller</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </section>

            <!-- Unsloth -->
            <section id="unsloth">
                <h2>1. Unsloth</h2>

                <div class="card">
                    <h3>Genel Bakis</h3>
                    <p>
                        Unsloth, fine-tuning islemini 2-5x hizlandiran ve bellek kullanimini %70'e kadar
                        azaltan optimize edilmis bir kutuphanedir. Ozellikle tuketici GPU'larda (RTX 40xx serisi)
                        mukemmel performans saglar.
                    </p>

                    <h4>Ozellikler</h4>
                    <ul>
                        <li>2-5x daha hizli egitim</li>
                        <li>%70 daha az GPU bellek kullanimi</li>
                        <li>4-bit quantization (QLoRA) destegi</li>
                        <li>Flash Attention 2 entegrasyonu</li>
                        <li>GGUF export (llama.cpp uyumlu)</li>
                        <li>100+ model destegi (Llama, Mistral, Phi, Gemma, Qwen vb.)</li>
                    </ul>

                    <h4>Kurulum</h4>
                    <pre><code># Basit kurulum
pip install unsloth

# Veya kaynak koddan (en son ozellikler)
pip install "unsloth @ git+https://github.com/unslothai/unsloth.git"</code></pre>

                    <h4>Temel Kullanim</h4>
                    <pre><code>from unsloth import FastLanguageModel

# Model yukle (4-bit quantized)
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Llama-3.2-3B-Instruct",
    max_seq_length=2048,
    load_in_4bit=True,
)

# LoRA adaptoru ekle
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
)

# Egitim icin hazirla
from trl import SFTTrainer
from transformers import TrainingArguments

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=5,
        max_steps=100,
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        output_dir="outputs",
    ),
)
trainer.train()</code></pre>

                    <h4>GPU Bellek Gereksinimleri (QLoRA)</h4>
                    <table>
                        <thead>
                            <tr>
                                <th>Model</th>
                                <th>Standart</th>
                                <th>Unsloth ile</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>7B</td>
                                <td>~16 GB</td>
                                <td>~6 GB</td>
                            </tr>
                            <tr>
                                <td>13B</td>
                                <td>~24 GB</td>
                                <td>~10 GB</td>
                            </tr>
                            <tr>
                                <td>70B</td>
                                <td>~80+ GB</td>
                                <td>~40 GB</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </section>

            <!-- Axolotl -->
            <section id="axolotl">
                <h2>2. Axolotl</h2>

                <div class="card">
                    <h3>Genel Bakis</h3>
                    <p>
                        Axolotl, YAML tabanli yapilandirma ile esnek fine-tuning saglayan guclu bir aractir.
                        Coklu GPU destegi, DeepSpeed entegrasyonu ve genis model yelpazesi ile profesyonel
                        kullanima uygundur.
                    </p>

                    <h4>Ozellikler</h4>
                    <ul>
                        <li>YAML tabanli yapilandirma</li>
                        <li>Coklu GPU ve dagitik egitim</li>
                        <li>DeepSpeed ZeRO entegrasyonu</li>
                        <li>Flash Attention 2 destegi</li>
                        <li>FSDP (Fully Sharded Data Parallel)</li>
                        <li>Genis dataset format destegi</li>
                        <li>WandB, MLflow entegrasyonu</li>
                    </ul>

                    <h4>Kurulum</h4>
                    <pre><code># pip ile kurulum
pip install axolotl

# Flash Attention (onerilen)
pip install flash-attn --no-build-isolation

# DeepSpeed (coklu GPU icin)
pip install deepspeed</code></pre>

                    <h4>YAML Yapilandirmasi</h4>
                    <pre><code># config.yaml
base_model: meta-llama/Llama-3.2-3B-Instruct
model_type: LlamaForCausalLM

load_in_4bit: true
adapter: qlora
lora_r: 32
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj

datasets:
  - path: dataset.json
    type: alpaca

sequence_len: 2048
sample_packing: true
pad_to_sequence_len: true

gradient_accumulation_steps: 4
micro_batch_size: 2
num_epochs: 3
learning_rate: 2e-4

optimizer: adamw_bnb_8bit
lr_scheduler: cosine
warmup_ratio: 0.03

output_dir: ./outputs
logging_steps: 1
save_steps: 100

bf16: auto
tf32: true
flash_attention: true
gradient_checkpointing: true</code></pre>

                    <h4>Calistirma</h4>
                    <pre><code># Tek GPU
axolotl train config.yaml

# Coklu GPU (2 GPU)
accelerate launch --num_processes 2 -m axolotl.cli.train config.yaml

# DeepSpeed ile
accelerate launch --config_file deepspeed_config.yaml -m axolotl.cli.train config.yaml</code></pre>
                </div>
            </section>

            <!-- Llama Factory -->
            <section id="llama-factory">
                <h2>3. Llama Factory</h2>

                <div class="card">
                    <h3>Genel Bakis</h3>
                    <p>
                        Llama Factory, web arayuzu ile kolay fine-tuning saglayan kullanici dostu bir aractir.
                        100'den fazla model destegi ve sifir kod ile egitim imkani sunar.
                    </p>

                    <h4>Ozellikler</h4>
                    <ul>
                        <li>Web UI (sifir kod)</li>
                        <li>100+ model destegi</li>
                        <li>CLI ve Python API</li>
                        <li>LoRA, QLoRA, Full fine-tuning</li>
                        <li>DPO, RLHF, PPO destegi</li>
                        <li>Coklu GPU destegi</li>
                    </ul>

                    <h4>Kurulum</h4>
                    <pre><code># pip ile kurulum
pip install llama-factory

# Veya kaynak koddan
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e ".[torch,metrics]"</code></pre>

                    <h4>Web UI Kullanimi</h4>
                    <pre><code># Web arayuzunu baslat
llamafactory-cli webui

# Tarayicida ac: http://localhost:7860</code></pre>
                    <p>Web UI'da:</p>
                    <ol>
                        <li>Model secin (Llama, Mistral, Qwen vb.)</li>
                        <li>Dataset yukleyin veya hazir dataset secin</li>
                        <li>Egitim parametrelerini ayarlayin</li>
                        <li>"Start" butonuna tiklayin</li>
                    </ol>

                    <h4>CLI Kullanimi</h4>
                    <pre><code># Egitim
llamafactory-cli train \
    --model_name_or_path meta-llama/Llama-3.2-3B-Instruct \
    --dataset alpaca_tr \
    --template llama3 \
    --finetuning_type lora \
    --lora_rank 16 \
    --output_dir ./output \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 4 \
    --learning_rate 2e-4 \
    --num_train_epochs 3

# Chat ile test
llamafactory-cli chat \
    --model_name_or_path meta-llama/Llama-3.2-3B-Instruct \
    --adapter_name_or_path ./output \
    --template llama3</code></pre>
                </div>
            </section>

            <!-- Torchtune -->
            <section id="torchtune">
                <h2>4. Torchtune (Meta Resmi)</h2>

                <div class="card">
                    <h3>Genel Bakis</h3>
                    <p>
                        Torchtune, Meta'nin resmi fine-tuning kutuphanesidir. Saf PyTorch kullanan,
                        mod√ºler ve genisletilebilir bir yapidir. Ozellikle Llama modelleri icin optimize edilmistir.
                    </p>

                    <h4>Ozellikler</h4>
                    <ul>
                        <li>Meta resmi destegi</li>
                        <li>Saf PyTorch (bagimliliksiz)</li>
                        <li>Moduler ve genisletilebilir</li>
                        <li>Full, LoRA, QLoRA destegi</li>
                        <li>FSDP ile dagitik egitim</li>
                        <li>Recipe (tarif) tabanli yapilandirma</li>
                    </ul>

                    <h4>Kurulum</h4>
                    <pre><code># pip ile kurulum
pip install torchtune

# Model indirme
tune download meta-llama/Llama-3.2-3B-Instruct \
    --output-dir ./models/llama-3.2-3b-instruct \
    --hf-token YOUR_HF_TOKEN</code></pre>

                    <h4>Mevcut Tarifler (Recipes)</h4>
                    <pre><code># Mevcut tarifleri listele
tune ls

# Cikti:
# full_finetune_single_device
# full_finetune_distributed
# lora_finetune_single_device
# lora_finetune_distributed
# qlora_finetune_single_device
# knowledge_distillation_single_device
# generate
# quantize</code></pre>

                    <h4>LoRA Fine-Tuning</h4>
                    <pre><code># Yapilandirma dosyasini kopyala
tune cp llama3_2/3B_lora_single_device ./my_config.yaml

# Yapilandirmayi duzenle ve calistir
tune run lora_finetune_single_device --config my_config.yaml

# Veya dogrudan parametrelerle
tune run lora_finetune_single_device \
    --config llama3_2/3B_lora_single_device \
    checkpointer.checkpoint_dir=./models/llama-3.2-3b-instruct \
    dataset.source=my_dataset \
    epochs=3</code></pre>

                    <h4>Yapilandirma Ornegi</h4>
                    <pre><code># my_config.yaml
model:
  _component_: torchtune.models.llama3_2.lora_llama3_2_3b
  lora_attn_modules: ['q_proj', 'k_proj', 'v_proj', 'output_proj']
  lora_rank: 16
  lora_alpha: 32

tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  path: ./models/llama-3.2-3b-instruct/tokenizer.model

dataset:
  _component_: torchtune.datasets.alpaca_dataset
  source: tatsu-lab/alpaca

optimizer:
  _component_: torch.optim.AdamW
  lr: 2e-4

loss:
  _component_: torch.nn.CrossEntropyLoss

epochs: 3
batch_size: 2
gradient_accumulation_steps: 4</code></pre>
                </div>
            </section>

            <!-- DeepSpeed -->
            <section id="deepspeed">
                <h2>5. DeepSpeed</h2>

                <div class="card">
                    <h3>Genel Bakis</h3>
                    <p>
                        DeepSpeed, Microsoft'un gelistirdigi buyuk olcekli model egitimi icin optimize
                        edilmis bir kutuphanedir. ZeRO optimizasyonu ile bellek kullanimini dramatik
                        sekilde azaltir.
                    </p>

                    <h4>Ozellikler</h4>
                    <ul>
                        <li>ZeRO-1, ZeRO-2, ZeRO-3 optimizasyonu</li>
                        <li>ZeRO-Offload (CPU/NVMe)</li>
                        <li>Mixed Precision (FP16, BF16)</li>
                        <li>Gradient Checkpointing</li>
                        <li>Pipeline Parallelism</li>
                        <li>Tensor Parallelism</li>
                    </ul>

                    <h4>Kurulum</h4>
                    <pre><code># pip ile kurulum
pip install deepspeed

# Dogrula
ds_report</code></pre>

                    <h4>ZeRO Seviyeleri</h4>
                    <table>
                        <thead>
                            <tr>
                                <th>Seviye</th>
                                <th>Paylasilan</th>
                                <th>Bellek Tasarrufu</th>
                                <th>Kullanim</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>ZeRO-1</td>
                                <td>Optimizer states</td>
                                <td>4x</td>
                                <td>Coklu GPU baslangic</td>
                            </tr>
                            <tr>
                                <td>ZeRO-2</td>
                                <td>+ Gradients</td>
                                <td>8x</td>
                                <td>Orta boyut modeller</td>
                            </tr>
                            <tr>
                                <td>ZeRO-3</td>
                                <td>+ Parameters</td>
                                <td>Linear</td>
                                <td>Buyuk modeller (70B+)</td>
                            </tr>
                        </tbody>
                    </table>

                    <h4>ZeRO-2 Yapilandirmasi</h4>
                    <pre><code>// ds_config_zero2.json
{
  "bf16": {
    "enabled": true
  },
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
      "device": "none"
    },
    "contiguous_gradients": true,
    "overlap_comm": true
  },
  "gradient_accumulation_steps": 4,
  "gradient_clipping": 1.0,
  "train_batch_size": "auto",
  "train_micro_batch_size_per_gpu": "auto"
}</code></pre>

                    <h4>ZeRO-3 + Offload Yapilandirmasi</h4>
                    <pre><code>// ds_config_zero3_offload.json
{
  "bf16": {
    "enabled": true
  },
  "zero_optimization": {
    "stage": 3,
    "offload_optimizer": {
      "device": "cpu",
      "pin_memory": true
    },
    "offload_param": {
      "device": "cpu",
      "pin_memory": true
    },
    "overlap_comm": true,
    "contiguous_gradients": true,
    "sub_group_size": 1e9
  },
  "gradient_accumulation_steps": 4,
  "gradient_clipping": 1.0,
  "train_batch_size": "auto",
  "train_micro_batch_size_per_gpu": "auto"
}</code></pre>

                    <h4>Hugging Face Transformers ile Kullanim</h4>
                    <pre><code>from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./output",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    learning_rate=2e-4,
    bf16=True,
    deepspeed="ds_config_zero2.json",  # DeepSpeed yapilandirmasi
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
)
trainer.train()</code></pre>
                </div>
            </section>

            <!-- Megatron-LM -->
            <section id="megatron">
                <h2>6. Megatron-LM</h2>

                <div class="card">
                    <h3>Genel Bakis</h3>
                    <p>
                        Megatron-LM, NVIDIA'nin buyuk dil modelleri egitimi icin gelistirdigi
                        kurumsal duzeyde bir kutuphanedir. Tensor, pipeline ve data parallelism
                        destegi ile cok buyuk modellerin egitimini mumkun kilar.
                    </p>

                    <h4>Ozellikler</h4>
                    <ul>
                        <li>Tensor Parallelism</li>
                        <li>Pipeline Parallelism</li>
                        <li>Data Parallelism</li>
                        <li>Mixed Precision (FP16, BF16)</li>
                        <li>Flash Attention entegrasyonu</li>
                        <li>Milyarlarca parametre destegi</li>
                    </ul>

                    <h4>Kurulum</h4>
                    <pre><code># Repository'yi klonla
git clone https://github.com/NVIDIA/Megatron-LM.git
cd Megatron-LM

# Bagimliliklari kur
pip install -r requirements.txt

# Apex kurulumu (NVIDIA)
git clone https://github.com/NVIDIA/apex.git
cd apex
pip install -v --no-cache-dir \
    --global-option="--cpp_ext" \
    --global-option="--cuda_ext" ./</code></pre>

                    <h4>Kullanim</h4>
                    <p>
                        Megatron-LM, kurumsal kullanim icin tasarlanmistir ve karmasik yapilandirma
                        gerektirir. Genellikle cok sayida GPU (8+) ile kullanilir.
                    </p>
                    <pre><code># Ornek: GPT egitimi (8 GPU)
python -m torch.distributed.launch \
    --nproc_per_node 8 \
    pretrain_gpt.py \
    --tensor-model-parallel-size 2 \
    --pipeline-model-parallel-size 2 \
    --num-layers 24 \
    --hidden-size 1024 \
    --num-attention-heads 16 \
    --micro-batch-size 4 \
    --global-batch-size 32 \
    --seq-length 2048 \
    --max-position-embeddings 2048 \
    --train-iters 100000 \
    --lr 1e-4 \
    --min-lr 1e-5 \
    --lr-decay-style cosine \
    --data-path ./data/my_dataset \
    --vocab-file ./vocab.json \
    --merge-file ./merges.txt \
    --bf16 \
    --save ./checkpoints \
    --save-interval 1000</code></pre>

                    <div class="alert alert-info">
                        <strong>Not:</strong> Megatron-LM, genellikle pre-training ve cok buyuk olcekli
                        fine-tuning icin kullanilir. Bireysel kullanicilar icin Unsloth veya Axolotl
                        daha uygun seceneklerdir.
                    </div>
                </div>
            </section>

            <!-- vLLM -->
            <section id="vllm">
                <h2>7. vLLM (Inference)</h2>

                <div class="card">
                    <h3>Genel Bakis</h3>
                    <p>
                        vLLM, fine-tuning degil inference (cikarim) icin kullanilir. Egittiginiz
                        modeli yuksek performansla sunmak icin idealdir. PagedAttention teknolojisi
                        ile yuksek throughput saglar.
                    </p>

                    <h4>Ozellikler</h4>
                    <ul>
                        <li>PagedAttention - verimli bellek yonetimi</li>
                        <li>Continuous batching</li>
                        <li>Tensor parallelism</li>
                        <li>OpenAI uyumlu API</li>
                        <li>LoRA adaptoru destegi</li>
                        <li>Quantization destegi (AWQ, GPTQ)</li>
                    </ul>

                    <h4>Kurulum</h4>
                    <pre><code>pip install vllm</code></pre>

                    <h4>Model Sunumu</h4>
                    <pre><code># OpenAI uyumlu API baslat
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-3.2-3B-Instruct \
    --port 8000

# LoRA adaptoru ile
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-3.2-3B-Instruct \
    --enable-lora \
    --lora-modules my_lora=./output/checkpoint \
    --port 8000

# API kullanimi
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "meta-llama/Llama-3.2-3B-Instruct",
        "messages": [{"role": "user", "content": "Merhaba!"}]
    }'</code></pre>
                </div>
            </section>

            <!-- Hangi Araci Secmeli -->
            <section>
                <h2>Hangi Araci Secmeliyim?</h2>

                <div class="card-grid">
                    <div class="card">
                        <h3>Yeni Basliyorum</h3>
                        <p><strong>Llama Factory</strong> - Web UI ile sifir kod</p>
                        <p>Veya <strong>Unsloth</strong> - basit API ile hizli sonuc</p>
                    </div>
                    <div class="card">
                        <h3>Hiz ve Verimlilik</h3>
                        <p><strong>Unsloth</strong> - 2-5x hizli, %70 az bellek</p>
                        <p>Tek GPU'da en iyi performans</p>
                    </div>
                    <div class="card">
                        <h3>Esneklik ve Kontrol</h3>
                        <p><strong>Axolotl</strong> - YAML config ile tam kontrol</p>
                        <p>Coklu GPU, DeepSpeed entegrasyonu</p>
                    </div>
                    <div class="card">
                        <h3>PyTorch Kullanicisi</h3>
                        <p><strong>Torchtune</strong> - Meta resmi, saf PyTorch</p>
                        <p>Moduler ve genisletilebilir</p>
                    </div>
                    <div class="card">
                        <h3>Buyuk Modeller (70B+)</h3>
                        <p><strong>DeepSpeed</strong> - ZeRO-3 + Offload</p>
                        <p>Sinirli GPU bellegi ile buyuk modeller</p>
                    </div>
                    <div class="card">
                        <h3>Kurumsal / Pre-training</h3>
                        <p><strong>Megatron-LM</strong> - NVIDIA resmi</p>
                        <p>Cok buyuk olcek, coklu node</p>
                    </div>
                </div>
            </section>

            <!-- Sonraki Adimlar -->
            <section>
                <h2>Sonraki Adimlar</h2>

                <div class="card-grid">
                    <div class="card">
                        <div class="card-icon">üìñ</div>
                        <h3>Fine-Tuning Rehberi</h3>
                        <p>Adim adim pratik fine-tuning ornekleri.</p>
                        <a href="fine-tuning-rehberi.html" class="btn btn-primary">Rehbere Git</a>
                    </div>
                    <div class="card">
                        <div class="card-icon">üîß</div>
                        <h3>Kurulum</h3>
                        <p>Ortaminizi henuz kurmadiysiniz.</p>
                        <a href="kurulum.html" class="btn btn-secondary">Kuruluma Don</a>
                    </div>
                </div>
            </section>
        </div>
    </main>

    <footer>
        <div class="container">
            <p>
                Kaynaklar:
                <a href="https://docs.unsloth.ai/" target="_blank">Unsloth</a> |
                <a href="https://github.com/axolotl-ai-cloud/axolotl" target="_blank">Axolotl</a> |
                <a href="https://github.com/hiyouga/LLaMA-Factory" target="_blank">Llama Factory</a> |
                <a href="https://pytorch.org/torchtune/" target="_blank">Torchtune</a> |
                <a href="https://www.deepspeed.ai/" target="_blank">DeepSpeed</a>
            </p>
        </div>
    </footer>
</body>
</html>

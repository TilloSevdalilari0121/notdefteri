<!DOCTYPE html>
<html lang="tr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Linux + NVIDIA Fine-Tuning Rehberi</title>
    <link rel="stylesheet" href="../css/style.css">
</head>
<body>
    <header>
        <div class="container">
            <h1>Linux + NVIDIA Fine-Tuning Rehberi</h1>
            <p>Adim Adim Pratik Fine-Tuning Ornekleri</p>
        </div>
    </header>

    <nav>
        <div class="container">
            <ul>
                <li><a href="../index.html">Ana Sayfa</a></li>
                <li><a href="index.html">Linux + NVIDIA</a></li>
                <li><a href="kurulum.html">Kurulum</a></li>
                <li><a href="araclar.html">Araclar</a></li>
                <li><a href="fine-tuning-rehberi.html" class="active">Fine-Tuning</a></li>
            </ul>
        </div>
    </nav>

    <div class="container">
        <div class="breadcrumb">
            <a href="../index.html">Ana Sayfa</a> &gt;
            <a href="index.html">Linux + NVIDIA</a> &gt; Fine-Tuning Rehberi
        </div>
    </div>

    <main>
        <div class="container">
            <!-- Giris -->
            <section>
                <h2>Baslamadan Once</h2>

                <div class="alert alert-info">
                    <strong>Gereksinimler:</strong> Bu rehberi takip etmek icin
                    <a href="kurulum.html">kurulum rehberini</a> tamamlamis olmalisiniz.
                    PyTorch CUDA ve fine-tuning araclari kurulu olmalidir.
                </div>

                <div class="card-grid">
                    <div class="card">
                        <h3>Bu Rehberde</h3>
                        <ul>
                            <li>Dataset hazirlama</li>
                            <li>Unsloth ile QLoRA fine-tuning</li>
                            <li>Axolotl ile fine-tuning</li>
                            <li>Llama Factory ile fine-tuning</li>
                            <li>Model export (GGUF)</li>
                            <li>Model test etme</li>
                        </ul>
                    </div>
                    <div class="card">
                        <h3>Kullanilacak Model</h3>
                        <p><strong>Llama 3.2 3B Instruct</strong></p>
                        <ul>
                            <li>3 milyar parametre</li>
                            <li>QLoRA ile ~6GB VRAM</li>
                            <li>Hizli egitim</li>
                            <li>Iyi performans</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- Dataset Hazirlama -->
            <section>
                <h2>1. Dataset Hazirlama</h2>

                <div class="step">
                    <div class="step-number">1.1</div>
                    <div class="step-content">
                        <h3>Dataset Formati</h3>
                        <p>Fine-tuning icin en yaygin format "Instruction-Input-Output" (Alpaca) formatidir:</p>
                        <pre><code>[
  {
    "instruction": "Asagidaki metni ozetle.",
    "input": "Python, 1991 yilinda Guido van Rossum tarafindan gelistirilmis...",
    "output": "Python, 1991'de olusturulmus yuksek seviyeli bir programlama dilidir."
  },
  {
    "instruction": "Bu soruyu yanitla.",
    "input": "Turkiye'nin baskenti neresidir?",
    "output": "Turkiye'nin baskenti Ankara'dir."
  }
]</code></pre>

                        <h4>Chat Formati (Llama 3)</h4>
                        <pre><code>[
  {
    "conversations": [
      {"role": "system", "content": "Sen yardimci bir asistansin."},
      {"role": "user", "content": "Python nedir?"},
      {"role": "assistant", "content": "Python, yuksek seviyeli bir programlama dilidir."}
    ]
  }
]</code></pre>
                    </div>
                </div>

                <div class="step">
                    <div class="step-number">1.2</div>
                    <div class="step-content">
                        <h3>Ornek Dataset Olusturma</h3>
                        <pre><code>import json

# Ornek Turkce dataset
dataset = [
    {
        "instruction": "Verilen konuyu acikla.",
        "input": "Yapay zeka nedir?",
        "output": "Yapay zeka (AI), bilgisayarlarin insan benzeri gorevleri yerine getirmesini saglayan teknolojiler butunudur. Makine ogrenimi, derin ogrenme ve dogal dil isleme gibi alt alanlari icerir."
    },
    {
        "instruction": "Soruyu yanitla.",
        "input": "Fine-tuning ne demek?",
        "output": "Fine-tuning, onceden egitilmis bir yapay zeka modelini belirli bir gorev veya alan icin ek verilerle yeniden egitme islemidir. Bu sayede model, genel yeteneklerini korurken ozel bir alanda daha iyi performans gosterir."
    },
    {
        "instruction": "Kodu acikla.",
        "input": "print('Merhaba Dunya')",
        "output": "Bu Python kodu, ekrana 'Merhaba Dunya' yazisini yazdirir. print() fonksiyonu, parantez icindeki metni konsola cikti olarak gosterir."
    },
    {
        "instruction": "Ceviri yap.",
        "input": "Hello, how are you?",
        "output": "Merhaba, nasilsin?"
    },
    {
        "instruction": "Ozet cikar.",
        "input": "Linux, 1991 yilinda Linus Torvalds tarafindan gelistirilmis acik kaynakli bir isletim sistemi cekirdesidir. Gunumuzde sunucularin buyuk cogunlugunda, Android cihazlarda ve bir√ßok gomulu sistemde kullanilmaktadir.",
        "output": "Linux, 1991'de Linus Torvalds'in gelistirdigi acik kaynakli bir isletim sistemi cekirdegi olup sunucular, Android ve gomulu sistemlerde yaygin kullanilir."
    }
]

# JSON olarak kaydet
with open("turkce_dataset.json", "w", encoding="utf-8") as f:
    json.dump(dataset, f, ensure_ascii=False, indent=2)

print(f"Dataset olusturuldu: {len(dataset)} ornek")</code></pre>
                    </div>
                </div>

                <div class="step">
                    <div class="step-number">1.3</div>
                    <div class="step-content">
                        <h3>Hugging Face Dataset Kullanimi</h3>
                        <pre><code>from datasets import load_dataset

# Hazir dataset yukle
dataset = load_dataset("databricks/databricks-dolly-15k", split="train")

# Veya yerel dosyadan
dataset = load_dataset("json", data_files="turkce_dataset.json", split="train")

# Dataset'i incele
print(dataset)
print(dataset[0])</code></pre>
                    </div>
                </div>
            </section>

            <!-- Unsloth ile Fine-Tuning -->
            <section>
                <h2>2. Unsloth ile Fine-Tuning (Onerilir)</h2>

                <div class="alert alert-success">
                    <strong>Neden Unsloth?</strong> 2-5x daha hizli egitim, %70 daha az bellek kullanimi,
                    otomatik optimizasyonlar. Tek GPU'da en iyi secim.
                </div>

                <div class="step">
                    <div class="step-number">2.1</div>
                    <div class="step-content">
                        <h3>Tam Fine-Tuning Scripti</h3>
                        <pre><code>import torch
from unsloth import FastLanguageModel
from datasets import load_dataset
from trl import SFTTrainer
from transformers import TrainingArguments

# =============================================================================
# 1. MODEL YUKLEME
# =============================================================================
print("Model yukleniyor...")

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Llama-3.2-3B-Instruct",
    max_seq_length=2048,
    dtype=None,  # Otomatik (bfloat16 veya float16)
    load_in_4bit=True,  # QLoRA icin 4-bit quantization
)

# =============================================================================
# 2. LORA ADAPTORU EKLEME
# =============================================================================
print("LoRA adaptoru ekleniyor...")

model = FastLanguageModel.get_peft_model(
    model,
    r=16,  # LoRA rank - dusuk = daha az parametre, yuksek = daha fazla kapasite
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0,  # Unsloth'da 0 optimize edilmis
    bias="none",
    use_gradient_checkpointing="unsloth",  # Bellek tasarrufu
    random_state=42,
)

# =============================================================================
# 3. DATASET HAZIRLAMA
# =============================================================================
print("Dataset hazirlaniyor...")

# Alpaca formati icin prompt template
alpaca_prompt = """Asagida bir gorevi tanimlayan bir talimat ve daha fazla
bagalam saglayan bir girdi bulunmaktadir. Istegi uygun sekilde tamamlayan
bir yanit yazin.

### Talimat:
{}

### Girdi:
{}

### Yanit:
{}"""

def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    inputs = examples["input"]
    outputs = examples["output"]
    texts = []
    for instruction, input_text, output in zip(instructions, inputs, outputs):
        text = alpaca_prompt.format(instruction, input_text, output) + tokenizer.eos_token
        texts.append(text)
    return {"text": texts}

# Dataset yukle
dataset = load_dataset("json", data_files="turkce_dataset.json", split="train")
dataset = dataset.map(formatting_prompts_func, batched=True)

print(f"Dataset boyutu: {len(dataset)} ornek")

# =============================================================================
# 4. EGITIM
# =============================================================================
print("Egitim basliyor...")

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=2048,
    dataset_num_proc=2,
    packing=False,  # Kisa ornekler icin True yapilabilir
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=5,
        num_train_epochs=3,
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=42,
        output_dir="outputs",
        save_strategy="epoch",
    ),
)

# GPU istatistikleri
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU: {gpu_stats.name}")
print(f"Maksimum bellek: {max_memory} GB")
print(f"Baslangic bellek kullanimi: {start_gpu_memory} GB")

# Egitimi baslat
trainer_stats = trainer.train()

# Egitim istatistikleri
used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
print(f"\nEgitim tamamlandi!")
print(f"Toplam egitim suresi: {trainer_stats.metrics['train_runtime']:.2f} saniye")
print(f"Maksimum bellek kullanimi: {used_memory} GB")

# =============================================================================
# 5. MODEL KAYDETME
# =============================================================================
print("\nModel kaydediliyor...")

# LoRA adaptorunu kaydet
model.save_pretrained("lora_model")
tokenizer.save_pretrained("lora_model")

# Merged model (opsiyonel - daha buyuk)
# model.save_pretrained_merged("merged_model", tokenizer, save_method="merged_16bit")

print("Model kaydedildi: ./lora_model")

# =============================================================================
# 6. GGUF EXPORT (llama.cpp icin)
# =============================================================================
print("\nGGUF export yapiliyor...")

# Q4_K_M - iyi denge (hiz/kalite)
model.save_pretrained_gguf("model_gguf", tokenizer, quantization_method="q4_k_m")

# Diger se√ßenekler:
# "q8_0" - Yuksek kalite
# "q5_k_m" - Orta kalite
# "q4_k_m" - Iyi denge
# "q3_k_m" - Kucuk boyut

print("GGUF export tamamlandi: ./model_gguf")
print("\n=== FINE-TUNING TAMAMLANDI ===")</code></pre>
                    </div>
                </div>

                <div class="step">
                    <div class="step-number">2.2</div>
                    <div class="step-content">
                        <h3>Scripti Calistirma</h3>
                        <pre><code># Virtual environment aktif et
source ~/venv-finetune/bin/activate

# Scripti calistir
python finetune_unsloth.py</code></pre>
                    </div>
                </div>

                <div class="step">
                    <div class="step-number">2.3</div>
                    <div class="step-content">
                        <h3>Modeli Test Etme</h3>
                        <pre><code>from unsloth import FastLanguageModel

# Egitilmis modeli yukle
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="lora_model",
    max_seq_length=2048,
    load_in_4bit=True,
)

# Inference icin hazirla
FastLanguageModel.for_inference(model)

# Test prompt
alpaca_prompt = """Asagida bir gorevi tanimlayan bir talimat ve daha fazla
bagalam saglayan bir girdi bulunmaktadir. Istegi uygun sekilde tamamlayan
bir yanit yazin.

### Talimat:
{}

### Girdi:
{}

### Yanit:
"""

inputs = tokenizer(
    [alpaca_prompt.format(
        "Soruyu yanitla.",
        "Yapay zeka nedir?",
        ""
    )],
    return_tensors="pt"
).to("cuda")

outputs = model.generate(**inputs, max_new_tokens=256, use_cache=True)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))</code></pre>
                    </div>
                </div>
            </section>

            <!-- Axolotl ile Fine-Tuning -->
            <section>
                <h2>3. Axolotl ile Fine-Tuning</h2>

                <div class="step">
                    <div class="step-number">3.1</div>
                    <div class="step-content">
                        <h3>YAML Yapilandirmasi</h3>
                        <pre><code># config.yaml
base_model: meta-llama/Llama-3.2-3B-Instruct
model_type: LlamaForCausalLM
tokenizer_type: AutoTokenizer

# Quantization
load_in_4bit: true
adapter: qlora
lora_r: 32
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
lora_target_linear: true

# Dataset
datasets:
  - path: turkce_dataset.json
    type: alpaca
    ds_type: json

# Sequence
sequence_len: 2048
sample_packing: true
pad_to_sequence_len: true

# Training
gradient_accumulation_steps: 4
micro_batch_size: 2
num_epochs: 3
learning_rate: 2e-4
train_on_inputs: false
group_by_length: false

# Optimizer
optimizer: adamw_bnb_8bit
lr_scheduler: cosine
warmup_ratio: 0.03

# Output
output_dir: ./axolotl_output
logging_steps: 1
save_steps: 100
eval_steps: 100
save_total_limit: 3

# Performance
bf16: auto
tf32: true
flash_attention: true
gradient_checkpointing: true

# Misc
special_tokens:
  pad_token: "<|end_of_text|>"</code></pre>
                    </div>
                </div>

                <div class="step">
                    <div class="step-number">3.2</div>
                    <div class="step-content">
                        <h3>Egitimi Baslat</h3>
                        <pre><code># Tek GPU
axolotl train config.yaml

# Coklu GPU (ornek: 2 GPU)
accelerate launch --num_processes 2 -m axolotl.cli.train config.yaml

# Preprocessing (buyuk datasetler icin)
axolotl preprocess config.yaml
axolotl train config.yaml --skip_preprocess</code></pre>
                    </div>
                </div>

                <div class="step">
                    <div class="step-number">3.3</div>
                    <div class="step-content">
                        <h3>Inference</h3>
                        <pre><code># Axolotl inference CLI
axolotl inference config.yaml --lora_model_dir ./axolotl_output

# Veya manuel
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.2-3B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
)
model = PeftModel.from_pretrained(base_model, "./axolotl_output")
tokenizer = AutoTokenizer.from_pretrained("./axolotl_output")</code></pre>
                    </div>
                </div>
            </section>

            <!-- Llama Factory ile Fine-Tuning -->
            <section>
                <h2>4. Llama Factory ile Fine-Tuning</h2>

                <div class="step">
                    <div class="step-number">4.1</div>
                    <div class="step-content">
                        <h3>Web UI ile (En Kolay)</h3>
                        <pre><code># Web arayuzunu baslat
llamafactory-cli webui

# Tarayicida ac: http://localhost:7860</code></pre>
                        <p>Web UI'da:</p>
                        <ol>
                            <li><strong>Model Path:</strong> meta-llama/Llama-3.2-3B-Instruct</li>
                            <li><strong>Adapter:</strong> LoRA</li>
                            <li><strong>Dataset:</strong> Custom dataset yukle</li>
                            <li><strong>Training Parameters:</strong> Ayarla</li>
                            <li><strong>Start:</strong> Egitimi baslat</li>
                        </ol>
                    </div>
                </div>

                <div class="step">
                    <div class="step-number">4.2</div>
                    <div class="step-content">
                        <h3>CLI ile</h3>
                        <pre><code># Dataset formatini ayarla (data/dataset_info.json)
{
  "turkce_custom": {
    "file_name": "turkce_dataset.json",
    "columns": {
      "prompt": "instruction",
      "query": "input",
      "response": "output"
    }
  }
}

# Egitim
llamafactory-cli train \
    --stage sft \
    --model_name_or_path meta-llama/Llama-3.2-3B-Instruct \
    --dataset turkce_custom \
    --dataset_dir ./data \
    --template llama3 \
    --finetuning_type lora \
    --lora_rank 16 \
    --lora_alpha 32 \
    --lora_target q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj \
    --output_dir ./llama_factory_output \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 4 \
    --lr_scheduler_type cosine \
    --learning_rate 2e-4 \
    --num_train_epochs 3 \
    --bf16 \
    --logging_steps 10 \
    --save_steps 100 \
    --plot_loss</code></pre>
                    </div>
                </div>

                <div class="step">
                    <div class="step-number">4.3</div>
                    <div class="step-content">
                        <h3>Chat ile Test</h3>
                        <pre><code># CLI chat
llamafactory-cli chat \
    --model_name_or_path meta-llama/Llama-3.2-3B-Instruct \
    --adapter_name_or_path ./llama_factory_output \
    --template llama3

# API server
llamafactory-cli api \
    --model_name_or_path meta-llama/Llama-3.2-3B-Instruct \
    --adapter_name_or_path ./llama_factory_output \
    --template llama3 \
    --port 8000</code></pre>
                    </div>
                </div>
            </section>

            <!-- Model Export -->
            <section>
                <h2>5. Model Export</h2>

                <div class="step">
                    <div class="step-number">5.1</div>
                    <div class="step-content">
                        <h3>GGUF Export (llama.cpp icin)</h3>
                        <pre><code># Unsloth ile (en kolay)
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained("lora_model")
model.save_pretrained_gguf("model_gguf", tokenizer, quantization_method="q4_k_m")

# Manuel llama.cpp ile
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make -j

# Hugging Face -> GGUF
python convert_hf_to_gguf.py ../merged_model --outfile model.gguf

# Quantize
./llama-quantize model.gguf model-q4_k_m.gguf Q4_K_M</code></pre>
                    </div>
                </div>

                <div class="step">
                    <div class="step-number">5.2</div>
                    <div class="step-content">
                        <h3>Hugging Face Hub'a Yukleme</h3>
                        <pre><code>from huggingface_hub import HfApi

api = HfApi()

# LoRA adaptoru yukle
api.upload_folder(
    folder_path="./lora_model",
    repo_id="kullanici_adi/model-lora",
    repo_type="model",
)

# GGUF yukle
api.upload_file(
    path_or_fileobj="./model_gguf/model-q4_k_m.gguf",
    path_in_repo="model-q4_k_m.gguf",
    repo_id="kullanici_adi/model-gguf",
    repo_type="model",
)</code></pre>
                    </div>
                </div>
            </section>

            <!-- vLLM ile Sunucu -->
            <section>
                <h2>6. Model Sunumu (vLLM)</h2>

                <div class="step">
                    <div class="step-number">6.1</div>
                    <div class="step-content">
                        <h3>vLLM Server Baslat</h3>
                        <pre><code># Merged model ile
python -m vllm.entrypoints.openai.api_server \
    --model ./merged_model \
    --port 8000

# LoRA adaptoru ile
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-3.2-3B-Instruct \
    --enable-lora \
    --lora-modules turkce_model=./lora_model \
    --port 8000</code></pre>
                    </div>
                </div>

                <div class="step">
                    <div class="step-number">6.2</div>
                    <div class="step-content">
                        <h3>API Kullanimi</h3>
                        <pre><code>import requests

response = requests.post(
    "http://localhost:8000/v1/chat/completions",
    json={
        "model": "turkce_model",  # veya merged model path
        "messages": [
            {"role": "system", "content": "Sen yardimci bir asistansin."},
            {"role": "user", "content": "Yapay zeka nedir?"}
        ],
        "max_tokens": 256,
        "temperature": 0.7
    }
)

print(response.json()["choices"][0]["message"]["content"])</code></pre>
                    </div>
                </div>
            </section>

            <!-- Performans Optimizasyonu -->
            <section>
                <h2>7. Performans Optimizasyonu</h2>

                <div class="card-grid">
                    <div class="card">
                        <h3>Bellek Yetersiz</h3>
                        <ul>
                            <li>Batch size'i azalt</li>
                            <li>Gradient accumulation artir</li>
                            <li>Sequence length kisalt</li>
                            <li>LoRA rank azalt (r=8)</li>
                            <li>Gradient checkpointing etkinlestir</li>
                        </ul>
                    </div>
                    <div class="card">
                        <h3>Daha Hizli Egitim</h3>
                        <ul>
                            <li>Flash Attention 2 kullan</li>
                            <li>BF16 (Ampere+ GPU'lar)</li>
                            <li>TF32 etkinlestir</li>
                            <li>Sample packing kullan</li>
                            <li>Unsloth kullan</li>
                        </ul>
                    </div>
                    <div class="card">
                        <h3>Daha Iyi Sonuclar</h3>
                        <ul>
                            <li>Dataset kalitesi en onemli</li>
                            <li>LoRA rank artir (r=32, 64)</li>
                            <li>Daha fazla epoch</li>
                            <li>Learning rate scheduling</li>
                            <li>Validation set kullan</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- Beklenen Sonuclar -->
            <section>
                <h2>8. Beklenen Sonuclar</h2>

                <div class="comparison-table">
                    <table>
                        <thead>
                            <tr>
                                <th>GPU</th>
                                <th>3B Model</th>
                                <th>7B Model</th>
                                <th>13B Model</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>RTX 4090 (24GB)</td>
                                <td>~5 dk</td>
                                <td>~15 dk</td>
                                <td>~30 dk</td>
                            </tr>
                            <tr>
                                <td>RTX 3090 (24GB)</td>
                                <td>~8 dk</td>
                                <td>~25 dk</td>
                                <td>~50 dk</td>
                            </tr>
                            <tr>
                                <td>A100 (80GB)</td>
                                <td>~3 dk</td>
                                <td>~8 dk</td>
                                <td>~15 dk</td>
                            </tr>
                            <tr>
                                <td>H100 (80GB)</td>
                                <td>~2 dk</td>
                                <td>~5 dk</td>
                                <td>~10 dk</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                <p><em>* QLoRA, 1000 ornek, 3 epoch, Unsloth ile</em></p>
            </section>

            <!-- Ozet -->
            <section>
                <h2>Ozet</h2>

                <div class="alert alert-success">
                    <strong>Tebrikler!</strong> Bu rehberi tamamladiyseniz:
                    <ul>
                        <li>Dataset hazirlama ogrendiniz</li>
                        <li>Unsloth ile QLoRA fine-tuning yaptiniz</li>
                        <li>Axolotl ve Llama Factory kullandiniz</li>
                        <li>Modelinizi GGUF formatina export ettiniz</li>
                        <li>vLLM ile model sundunuz</li>
                    </ul>
                </div>

                <div class="card-grid">
                    <div class="card">
                        <div class="card-icon">üìö</div>
                        <h3>Daha Fazla Bilgi</h3>
                        <p>Araclarin detayli belgelerine bakin.</p>
                        <a href="araclar.html" class="btn btn-secondary">Araclara Git</a>
                    </div>
                    <div class="card">
                        <div class="card-icon">üè†</div>
                        <h3>Diger Platformlar</h3>
                        <p>AMD veya Windows icin rehberlere bakin.</p>
                        <a href="../index.html" class="btn btn-secondary">Ana Sayfa</a>
                    </div>
                </div>
            </section>
        </div>
    </main>

    <footer>
        <div class="container">
            <p>
                Kaynaklar:
                <a href="https://docs.unsloth.ai/" target="_blank">Unsloth</a> |
                <a href="https://github.com/axolotl-ai-cloud/axolotl" target="_blank">Axolotl</a> |
                <a href="https://github.com/hiyouga/LLaMA-Factory" target="_blank">Llama Factory</a> |
                <a href="https://docs.vllm.ai/" target="_blank">vLLM</a>
            </p>
        </div>
    </footer>
</body>
</html>

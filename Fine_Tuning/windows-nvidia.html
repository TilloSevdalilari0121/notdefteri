<!DOCTYPE html>
<html lang="tr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Windows + NVIDIA GPU Fine-Tuning Rehberi</title>
    <link rel="stylesheet" href="css/style.css">
</head>
<body>
    <header>
        <div class="container">
            <h1>Windows + NVIDIA GPU Fine-Tuning Rehberi</h1>
            <p>CUDA ile Tam Destekli Fine-Tuning - En Kolay Yol</p>
        </div>
    </header>

    <nav>
        <a href="index.html">Ana Sayfa</a>
        <a href="#gereksinimler">Gereksinimler</a>
        <a href="#kurulum">Kurulum</a>
        <a href="#egitim">Egitim</a>
        <a href="#unsloth">Unsloth</a>
    </nav>

    <main>
        <div class="container">

            <div class="alert alert-success">
                <strong>En Kolay Platform!</strong>
                Windows + NVIDIA kombinasyonu en iyi desteklenen platformdur. Tum araclar sorunsuz calisir.
                Bu rehberi takip ederek 30 dakikada ilk fine-tuning'inizi yapabilirsiniz.
            </div>

            <!-- ICINDEKILER -->
            <div class="toc">
                <h3>Bu Rehberde</h3>
                <ol>
                    <li><a href="#gereksinimler">Gereksinimler</a></li>
                    <li><a href="#kurulum">CUDA ve PyTorch Kurulumu</a></li>
                    <li><a href="#egitim">Fine-Tuning Yapma</a></li>
                    <li><a href="#unsloth">Unsloth ile Hizli Egitim</a></li>
                    <li><a href="#export">Model Export</a></li>
                    <li><a href="#sorunlar">Sorun Giderme</a></li>
                </ol>
            </div>

            <!-- BOLUM 1: GEREKSINIMLER -->
            <section id="gereksinimler">
                <h2>1. Gereksinimler</h2>

                <div class="requirements">
                    <h4>Sistem Gereksinimleri</h4>
                    <ul>
                        <li>Windows 10 veya 11</li>
                        <li>NVIDIA GPU (RTX 20xx, 30xx, 40xx)</li>
                        <li>En az 16 GB RAM</li>
                        <li>30 GB bos disk alani</li>
                    </ul>
                </div>

                <h3>GPU Bellek Gereksinimleri</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Model Boyutu</th>
                            <th>QLoRA (4-bit)</th>
                            <th>LoRA (16-bit)</th>
                            <th>Onerilen GPU</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>3B (Llama 3.2)</td>
                            <td>6 GB</td>
                            <td>12 GB</td>
                            <td>RTX 3060 12GB+</td>
                        </tr>
                        <tr>
                            <td>7B (Llama 2, Mistral)</td>
                            <td>8 GB</td>
                            <td>16 GB</td>
                            <td>RTX 3070 8GB+</td>
                        </tr>
                        <tr>
                            <td>13B</td>
                            <td>12 GB</td>
                            <td>28 GB</td>
                            <td>RTX 3090/4090</td>
                        </tr>
                        <tr>
                            <td>70B</td>
                            <td>40 GB</td>
                            <td>140+ GB</td>
                            <td>A100 / Coklu GPU</td>
                        </tr>
                    </tbody>
                </table>

                <div class="alert alert-info">
                    <strong>GPU'nuzu Ogrenin:</strong>
                    <code>Win + R</code> tuslayin, <code>dxdiag</code> yazin, "Display" sekmesine bakin.
                    Veya <code>nvidia-smi</code> komutunu calistirin.
                </div>
            </section>

            <!-- BOLUM 2: KURULUM -->
            <section id="kurulum">
                <h2>2. CUDA ve PyTorch Kurulumu</h2>

                <!-- ADIM 1 -->
                <div class="step">
                    <div class="step-header">
                        <div class="step-number">1</div>
                        <div class="step-title">NVIDIA Suruculerini Guncelle</div>
                    </div>
                    <p>
                        <a href="https://www.nvidia.com/Download/index.aspx" target="_blank">NVIDIA Driver Download</a>
                        sayfasindan GPU'nuza uygun en son surucuyu indirin ve kurun.
                    </p>
                    <p>Kurulumdan sonra komut satiri (cmd) ac ve kontrol et:</p>
                    <pre>nvidia-smi</pre>

                    <div class="expected-output">
                        <pre>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 545.xx       Driver Version: 545.xx       CUDA Version: 12.3    |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  WDDM | 00000000:01:00.0  On |                  N/A |
|  0%   35C    P8    10W / 320W |    512MiB / 24576MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+</pre>
                    </div>

                    <p><strong>"CUDA Version: 12.x"</strong> goruyorsaniz surucu dogru kurulu.</p>
                </div>

                <!-- ADIM 2 -->
                <div class="step">
                    <div class="step-header">
                        <div class="step-number">2</div>
                        <div class="step-title">Python Kur</div>
                    </div>
                    <p>
                        <a href="https://www.python.org/downloads/" target="_blank">Python.org</a>'dan
                        Python 3.11 indirin ve kurun.
                    </p>
                    <div class="alert alert-warning">
                        <strong>Onemli:</strong> Kurulum sirasinda <strong>"Add Python to PATH"</strong>
                        kutusunu isaretleyin!
                    </div>

                    <p>Kurulumu dogrula:</p>
                    <pre>python --version</pre>

                    <div class="expected-output">
                        <pre>Python 3.11.x</pre>
                    </div>
                </div>

                <!-- ADIM 3 -->
                <div class="step">
                    <div class="step-header">
                        <div class="step-number">3</div>
                        <div class="step-title">Calisma Klasoru ve Virtual Environment</div>
                    </div>

                    <p>Komut satiri (cmd) acin ve su komutlari calistirin:</p>
                    <pre>mkdir C:\fine-tuning
cd C:\fine-tuning

python -m venv venv
venv\Scripts\activate

pip install --upgrade pip</pre>

                    <p class="cmd-explain">
                        <strong>Ne yapiyoruz:</strong> C:\fine-tuning klasoru olusturduk,
                        icinde izole bir Python ortami (venv) olusturduk ve aktif ettik.
                    </p>

                    <div class="alert alert-info">
                        <strong>Her seferinde:</strong> Yeni terminal actiginizda
                        <code>cd C:\fine-tuning</code> ve <code>venv\Scripts\activate</code>
                        komutlarini calistirmaniz gerekir.
                    </div>
                </div>

                <!-- ADIM 4 -->
                <div class="step">
                    <div class="step-header">
                        <div class="step-number">4</div>
                        <div class="step-title">PyTorch CUDA Kur</div>
                    </div>

                    <pre>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124</pre>

                    <p class="cmd-explain">
                        Bu komut PyTorch'u CUDA 12.4 destegi ile kurar. Eger farkli CUDA surumunuz varsa
                        <a href="https://pytorch.org/get-started/locally/" target="_blank">pytorch.org</a>'dan
                        dogru komutu alin.
                    </p>

                    <p><strong>Kurulumu test et:</strong></p>
                    <pre>python -c "import torch; print('PyTorch:', torch.__version__); print('CUDA:', torch.cuda.is_available()); print('GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'YOK')"</pre>

                    <div class="expected-output">
                        <pre>PyTorch: 2.4.0+cu124
CUDA: True
GPU: NVIDIA GeForce RTX 4090</pre>
                    </div>

                    <div class="alert alert-danger">
                        <strong>CUDA: False ise</strong>
                        <ol>
                            <li>nvidia-smi calistigini kontrol edin</li>
                            <li>PyTorch'u dogru CUDA surumu ile kurdugunuzdan emin olun</li>
                            <li>Bilgisayari yeniden baslatin</li>
                        </ol>
                    </div>
                </div>

                <!-- ADIM 5 -->
                <div class="step">
                    <div class="step-header">
                        <div class="step-number">5</div>
                        <div class="step-title">Fine-Tuning Paketlerini Kur</div>
                    </div>

                    <pre>pip install transformers datasets accelerate peft trl bitsandbytes sentencepiece huggingface_hub</pre>

                    <p>Kurulumu dogrula:</p>
                    <pre>python -c "import transformers, peft, trl, bitsandbytes; print('Tum paketler kurulu!')"</pre>
                </div>

                <!-- ADIM 6 -->
                <div class="step">
                    <div class="step-header">
                        <div class="step-number">6</div>
                        <div class="step-title">Hugging Face Giris</div>
                    </div>

                    <p>Bazi modeller (Llama gibi) icin Hugging Face hesabi gerekir:</p>
                    <ol>
                        <li><a href="https://huggingface.co/join" target="_blank">huggingface.co</a>'da hesap olustur</li>
                        <li><a href="https://huggingface.co/settings/tokens" target="_blank">Token olustur</a> (Read izni yeterli)</li>
                        <li>Terminalde giris yap:</li>
                    </ol>
                    <pre>huggingface-cli login</pre>
                    <p>Token'inizi yapistirun (gozukmez, normal).</p>
                </div>

                <div class="alert alert-success">
                    <strong>Kurulum Tamamlandi!</strong>
                    Artik fine-tuning yapabilirsiniz.
                </div>
            </section>

            <!-- BOLUM 3: EGITIM -->
            <section id="egitim">
                <h2>3. Fine-Tuning Yapma</h2>

                <!-- ADIM 1 -->
                <div class="step">
                    <div class="step-header">
                        <div class="step-number">1</div>
                        <div class="step-title">Dataset Dosyasi Olustur</div>
                    </div>

                    <p>C:\fine-tuning klasorunde <code>dataset.json</code> dosyasi olusturun (Not Defteri ile):</p>

                    <pre>[
  {
    "instruction": "Soruyu yanitla.",
    "input": "Python nedir?",
    "output": "Python, 1991 yilinda Guido van Rossum tarafindan gelistirilmis, okunmasi kolay ve cok amacli bir programlama dilidir. Web gelistirme, veri bilimi, yapay zeka ve otomasyon gibi bir√ßok alanda kullanilir."
  },
  {
    "instruction": "Ceviri yap.",
    "input": "Hello, how are you?",
    "output": "Merhaba, nasilsin?"
  },
  {
    "instruction": "Kod acikla.",
    "input": "print('Merhaba Dunya')",
    "output": "Bu Python kodu ekrana 'Merhaba Dunya' yazisini yazdirir. print() fonksiyonu, parantez icindeki degeri konsola cikti olarak gosterir."
  },
  {
    "instruction": "Ozet cikar.",
    "input": "Fine-tuning, onceden egitilmis bir yapay zeka modelini belirli bir gorev veya veri seti uzerinde yeniden egitme islemidir. Bu islem, modelin genel yeteneklerini korurken belirli bir alanda uzmanlasmassini saglar. Ozellikle buyuk dil modellerinde yaygin kullanilir.",
    "output": "Fine-tuning, hazir bir AI modelini ozel verilerle yeniden egiterek belirli bir alanda uzmanlasstirma islemidir."
  },
  {
    "instruction": "Liste olustur.",
    "input": "Python ogrenme kaynaklari",
    "output": "1. Python resmi dokumantasyonu (docs.python.org)\n2. Codecademy Python kursu\n3. Real Python web sitesi\n4. Automate the Boring Stuff kitabi\n5. YouTube egitim videolari"
  }
]</pre>

                    <div class="alert alert-info">
                        <strong>Dataset Formati:</strong>
                        <ul>
                            <li><code>instruction</code>: Modele ne yapmasi gerektigini soyler</li>
                            <li><code>input</code>: Islenecek girdi verisi</li>
                            <li><code>output</code>: Modelin vermesi gereken cevap</li>
                        </ul>
                    </div>
                </div>

                <!-- ADIM 2 -->
                <div class="step">
                    <div class="step-header">
                        <div class="step-number">2</div>
                        <div class="step-title">Egitim Scriptini Olustur</div>
                    </div>

                    <p>C:\fine-tuning klasorunde <code>train.py</code> dosyasi olusturun:</p>

                    <pre>"""
Windows + NVIDIA Fine-Tuning Script
"""

import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    BitsAndBytesConfig,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer

# ==============================================================
# AYARLAR - Ihtiyacina gore degistir
# ==============================================================
MODEL_ID = "meta-llama/Llama-3.2-3B-Instruct"  # Kullanilacak model
DATASET_FILE = "dataset.json"                   # Dataset dosyasi
OUTPUT_DIR = "./output"                         # Cikti klasoru
EPOCHS = 3                                      # Egitim tur sayisi
BATCH_SIZE = 1                                  # GPU bellegine gore ayarla
LEARNING_RATE = 2e-4                            # Ogrenme hizi
MAX_LENGTH = 512                                # Maksimum token uzunlugu

# ==============================================================
# 1. GPU KONTROL
# ==============================================================
print("=" * 60)
print("GPU KONTROL")
print("=" * 60)

if not torch.cuda.is_available():
    print("HATA: CUDA bulunamadi!")
    print("NVIDIA suruculerini ve PyTorch CUDA kurulumunu kontrol edin.")
    exit(1)

print(f"PyTorch: {torch.__version__}")
print(f"CUDA: {torch.version.cuda}")
print(f"GPU: {torch.cuda.get_device_name(0)}")
print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
print()

# ==============================================================
# 2. MODEL YUKLEME
# ==============================================================
print("=" * 60)
print("MODEL YUKLENIYOR")
print("=" * 60)

# 4-bit quantization (GPU bellegi tasarrufu)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

print(f"Model yuklendi: {MODEL_ID}")
print()

# ==============================================================
# 3. LORA AYARLARI
# ==============================================================
print("=" * 60)
print("LORA HAZIRLANIYOR")
print("=" * 60)

model = prepare_model_for_kbit_training(model)

lora_config = LoraConfig(
    r=16,                   # LoRA rank
    lora_alpha=32,          # LoRA alpha
    target_modules=[        # Egitilecek katmanlar
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
print()

# ==============================================================
# 4. DATASET YUKLEME
# ==============================================================
print("=" * 60)
print("DATASET YUKLENIYOR")
print("=" * 60)

dataset = load_dataset("json", data_files=DATASET_FILE, split="train")
print(f"Ornek sayisi: {len(dataset)}")

def format_prompt(example):
    return {
        "text": f"""### Talimat:
{example['instruction']}

### Girdi:
{example['input']}

### Cevap:
{example['output']}</s>"""
    }

dataset = dataset.map(format_prompt)
print()

# ==============================================================
# 5. EGITIM
# ==============================================================
print("=" * 60)
print("EGITIM BASLIYOR")
print("=" * 60)

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=4,
    learning_rate=LEARNING_RATE,
    weight_decay=0.01,
    warmup_ratio=0.03,
    lr_scheduler_type="cosine",
    logging_steps=1,
    save_strategy="epoch",
    fp16=True,
    optim="adamw_torch",
    report_to="none",
)

trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    tokenizer=tokenizer,
    dataset_text_field="text",
    max_seq_length=MAX_LENGTH,
)

# Egitim
trainer.train()

# ==============================================================
# 6. KAYDETME
# ==============================================================
print()
print("=" * 60)
print("MODEL KAYDEDILIYOR")
print("=" * 60)

trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print(f"Model kaydedildi: {OUTPUT_DIR}")
print()
print("=" * 60)
print("EGITIM TAMAMLANDI!")
print("=" * 60)</pre>
                </div>

                <!-- ADIM 3 -->
                <div class="step">
                    <div class="step-header">
                        <div class="step-number">3</div>
                        <div class="step-title">Egitimi Baslat</div>
                    </div>

                    <pre>cd C:\fine-tuning
venv\Scripts\activate
python train.py</pre>

                    <p>Ilk calistirmada model indirilecek (~6 GB), biraz bekleyin.</p>

                    <div class="expected-output">
                        <pre>============================================================
GPU KONTROL
============================================================
PyTorch: 2.4.0+cu124
CUDA: 12.4
GPU: NVIDIA GeForce RTX 4090
VRAM: 24.0 GB

============================================================
MODEL YUKLENIYOR
============================================================
Model yuklendi: meta-llama/Llama-3.2-3B-Instruct

============================================================
LORA HAZIRLANIYOR
============================================================
trainable params: 13,631,488 || all params: 1,713,540,608 || trainable%: 0.80

============================================================
DATASET YUKLENIYOR
============================================================
Ornek sayisi: 5

============================================================
EGITIM BASLIYOR
============================================================
{'loss': 2.45, 'learning_rate': 0.0002, 'epoch': 0.6}
...

============================================================
EGITIM TAMAMLANDI!
============================================================</pre>
                    </div>
                </div>

                <!-- ADIM 4 -->
                <div class="step">
                    <div class="step-header">
                        <div class="step-number">4</div>
                        <div class="step-title">Modeli Test Et</div>
                    </div>

                    <p><code>test.py</code> dosyasi olusturun:</p>

                    <pre>"""Egitilmis modeli test et"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

print("Model yukleniyor...")

# Base model
base_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.2-3B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
)

# Egitilmis LoRA adaptoru
model = PeftModel.from_pretrained(base_model, "./output")
tokenizer = AutoTokenizer.from_pretrained("./output")

model.eval()
print("Model haziir!\n")

# Test
def test(instruction, input_text):
    prompt = f"""### Talimat:
{instruction}

### Girdi:
{input_text}

### Cevap:
"""
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=200,
            temperature=0.7,
            do_sample=True,
        )

    result = tokenizer.decode(output[0], skip_special_tokens=True)
    # Sadece cevabi goster
    if "### Cevap:" in result:
        result = result.split("### Cevap:")[-1].strip()
    return result

# Test ornekleri
print("=" * 50)
print("TEST 1")
print("=" * 50)
print(test("Soruyu yanitla.", "Fine-tuning nedir?"))
print()

print("=" * 50)
print("TEST 2")
print("=" * 50)
print(test("Ceviri yap.", "I love programming."))
print()</pre>

                    <p>Calistir:</p>
                    <pre>python test.py</pre>
                </div>
            </section>

            <!-- BOLUM 4: UNSLOTH -->
            <section id="unsloth">
                <h2>4. Unsloth ile Hizli Egitim (Onerilen)</h2>

                <div class="alert alert-success">
                    <strong>Unsloth Avantajlari:</strong>
                    <ul>
                        <li>2-5x daha hizli egitim</li>
                        <li>%70 daha az GPU bellegi</li>
                        <li>Otomatik optimizasyonlar</li>
                        <li>Kolay GGUF export</li>
                    </ul>
                </div>

                <div class="step">
                    <div class="step-header">
                        <div class="step-number">1</div>
                        <div class="step-title">Unsloth Kur</div>
                    </div>
                    <pre>pip install unsloth</pre>
                </div>

                <div class="step">
                    <div class="step-header">
                        <div class="step-number">2</div>
                        <div class="step-title">Unsloth ile Egitim</div>
                    </div>

                    <p><code>train_unsloth.py</code> dosyasi olusturun:</p>

                    <pre>"""Unsloth ile Hizli Fine-Tuning"""

import torch
from unsloth import FastLanguageModel
from datasets import load_dataset
from trl import SFTTrainer
from transformers import TrainingArguments

# Model yukle (4-bit, optimize edilmis)
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Llama-3.2-3B-Instruct",
    max_seq_length=2048,
    load_in_4bit=True,
)

# LoRA ekle
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
)

# Dataset
dataset = load_dataset("json", data_files="dataset.json", split="train")

def format_prompt(example):
    return {"text": f"""### Talimat:
{example['instruction']}

### Girdi:
{example['input']}

### Cevap:
{example['output']}{tokenizer.eos_token}"""}

dataset = dataset.map(format_prompt)

# Egitim
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=2048,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=5,
        num_train_epochs=3,
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=1,
        output_dir="output_unsloth",
        optim="adamw_8bit",
    ),
)

print("Egitim basliyor...")
trainer.train()

# Kaydet
model.save_pretrained("output_unsloth")
tokenizer.save_pretrained("output_unsloth")

# GGUF export (llama.cpp icin)
print("\nGGUF export yapiliyor...")
model.save_pretrained_gguf("model_gguf", tokenizer, quantization_method="q4_k_m")

print("\nTamamlandi!")
print("LoRA model: output_unsloth/")
print("GGUF model: model_gguf/")</pre>

                    <pre>python train_unsloth.py</pre>
                </div>
            </section>

            <!-- BOLUM 5: EXPORT -->
            <section id="export">
                <h2>5. Model Export</h2>

                <h3>GGUF Export (Ollama, llama.cpp icin)</h3>
                <p>Unsloth kullandiyseniz GGUF otomatik olusturuldu. Manuel yapmak icin:</p>

                <pre># llama.cpp indir
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

# GGUF'a cevir
pip install -r requirements.txt
python convert_hf_to_gguf.py ../output --outfile model.gguf

# Quantize (boyut kucultme)
# Windows icin llama.cpp releases'ten derlenimis exe indir
llama-quantize.exe model.gguf model-q4_k_m.gguf Q4_K_M</pre>

                <h3>Ollama'da Kullanma</h3>
                <pre># Modelfile olustur
echo "FROM ./model-q4_k_m.gguf" > Modelfile

# Ollama'ya ekle
ollama create benim-modelim -f Modelfile

# Calistir
ollama run benim-modelim</pre>
            </section>

            <!-- BOLUM 6: SORUN GIDERME -->
            <section id="sorunlar">
                <h2>6. Sorun Giderme</h2>

                <h3>CUDA bulunamadi</h3>
                <pre># nvidia-smi calisiyior mu?
nvidia-smi

# PyTorch CUDA surumu
python -c "import torch; print(torch.version.cuda)"

# Dogru PyTorch kur
pip uninstall torch -y
pip install torch --index-url https://download.pytorch.org/whl/cu124</pre>

                <h3>Out of Memory</h3>
                <pre># train.py'de su degerleri dusur:
BATCH_SIZE = 1
MAX_LENGTH = 256

# Veya gradient_accumulation_steps artir:
gradient_accumulation_steps=8</pre>

                <h3>Model indirme hatasi</h3>
                <pre># Hugging Face token'i kontrol et
huggingface-cli whoami

# Yeniden giris yap
huggingface-cli login</pre>
            </section>

            <div class="alert alert-success">
                <strong>Tebrikler!</strong>
                Windows + NVIDIA GPU ile fine-tuning rehberini tamamladiniz.
            </div>

        </div>
    </main>

    <footer>
        <div class="container">
            <p><a href="index.html">Ana Sayfa</a> | <a href="linux-nvidia.html">Linux + NVIDIA</a></p>
        </div>
    </footer>
</body>
</html>

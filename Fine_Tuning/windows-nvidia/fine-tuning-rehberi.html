<!DOCTYPE html>
<html lang="tr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Windows + NVIDIA Adim Adim Fine-Tuning Rehberi</title>
    <link rel="stylesheet" href="../css/style.css">
</head>
<body>
    <header>
        <div class="container">
            <h1>Adim Adim Fine-Tuning Rehberi</h1>
            <p>Windows + NVIDIA ile Unsloth Kullanarak LLM Fine-Tuning</p>
        </div>
    </header>

    <nav>
        <div class="container">
            <ul>
                <li><a href="../index.html">Ana Sayfa</a></li>
                <li><a href="../windows-amd/index.html">Windows + AMD</a></li>
                <li><a href="../linux-amd/index.html">Linux + AMD</a></li>
                <li><a href="index.html" class="active">Windows + NVIDIA</a></li>
                <li><a href="../linux-nvidia/index.html">Linux + NVIDIA</a></li>
            </ul>
        </div>
    </nav>

    <div class="container">
        <div class="breadcrumb">
            <a href="../index.html">Ana Sayfa</a> &gt; <a href="index.html">Windows + NVIDIA</a> &gt; Fine-Tuning Rehberi
        </div>
    </div>

    <main>
        <div class="container">
            <!-- Unsloth ile Fine-Tuning -->
            <section>
                <h2>Unsloth ile Fine-Tuning (Onerilen)</h2>

                <div class="alert alert-info">
                    <strong>Onkosul:</strong> <a href="kurulum.html">Kurulum Rehberi</a>'ni tamamlayin.
                </div>

                <h3>Tam Egitim Scripti</h3>
                <pre><code># train_unsloth.py
from unsloth import FastLanguageModel
import torch
from datasets import load_dataset
from trl import SFTTrainer, SFTConfig

# ===== AYARLAR =====
MODEL_NAME = "unsloth/Llama-3.2-3B"  # veya "unsloth/Mistral-7B-v0.3"
MAX_SEQ_LENGTH = 2048
OUTPUT_DIR = "./outputs/llama3-finetuned"

# ===== MODEL YUKLE =====
print("Model yukleniyor...")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=MODEL_NAME,
    max_seq_length=MAX_SEQ_LENGTH,
    dtype=None,  # Otomatik sec
    load_in_4bit=True,  # QLoRA icin
)

# ===== LORA EKLE =====
print("LoRA ekleniyor...")
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=42,
)

# ===== VERI SETI =====
print("Veri seti yukleniyor...")
dataset = load_dataset("tatsu-lab/alpaca", split="train[:2000]")

# Formatlama
alpaca_prompt = """### Instruction:
{instruction}

### Input:
{input}

### Response:
{output}"""

def formatting_func(examples):
    texts = []
    for instruction, input_text, output in zip(
        examples["instruction"],
        examples["input"],
        examples["output"]
    ):
        text = alpaca_prompt.format(
            instruction=instruction,
            input=input_text if input_text else "",
            output=output
        ) + tokenizer.eos_token
        texts.append(text)
    return {"text": texts}

dataset = dataset.map(formatting_func, batched=True)

# ===== EGITIM =====
print("Egitim basliyor...")
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=MAX_SEQ_LENGTH,
    dataset_num_proc=1,  # Windows icin
    packing=False,
    args=SFTConfig(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=5,
        num_train_epochs=1,
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=10,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=42,
        output_dir=OUTPUT_DIR,
        save_strategy="epoch",
    ),
)

trainer_stats = trainer.train()

# ===== KAYDET =====
print("Model kaydediliyor...")
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print(f"Tamamlandi! Model: {OUTPUT_DIR}")
print(f"Egitim suresi: {trainer_stats.metrics['train_runtime']:.2f} saniye")</code></pre>

                <p>Calistirmak icin:</p>
                <pre><code>python train_unsloth.py</code></pre>
            </section>

            <!-- Model Test -->
            <section>
                <h2>Modeli Test Etme</h2>

                <pre><code># test_model.py
from unsloth import FastLanguageModel

# Model yukle
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="./outputs/llama3-finetuned",
    max_seq_length=2048,
    load_in_4bit=True,
)

# Inference modu
FastLanguageModel.for_inference(model)

def generate(instruction, input_text=""):
    prompt = f"""### Instruction:
{instruction}

### Input:
{input_text}

### Response:
"""
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

    outputs = model.generate(
        **inputs,
        max_new_tokens=256,
        temperature=0.7,
        do_sample=True,
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.split("### Response:")[-1].strip()

# Test
print(generate("Python nedir? Kisa acikla."))
print("-" * 50)
print(generate("Asagidaki cumleyi Ingilizce'ye cevir.", "Bugun hava cok guzel."))</code></pre>
            </section>

            <!-- GGUF Export -->
            <section>
                <h2>GGUF Formatina Donusturme</h2>

                <p>Modelinizi llama.cpp veya Ollama ile kullanmak icin GGUF'a donusturun:</p>

                <pre><code># Unsloth ile GGUF export (beta)
model.save_pretrained_gguf(
    "outputs/llama3-gguf",
    tokenizer,
    quantization_method="q4_k_m"  # veya "q8_0", "f16"
)</code></pre>

                <h3>Alternatif: llama.cpp ile</h3>
                <pre><code># Onca modeli birlestirin
model.save_pretrained_merged("outputs/merged", tokenizer)

# llama.cpp convert.py kullanin
python llama.cpp/convert_hf_to_gguf.py outputs/merged --outfile model.gguf</code></pre>
            </section>

            <!-- Hugging Face Hub -->
            <section>
                <h2>Hugging Face Hub'a Yukleme</h2>

                <pre><code># Hub'a yukle
model.push_to_hub("kullanici-adi/model-adi", token="hf_...")
tokenizer.push_to_hub("kullanici-adi/model-adi", token="hf_...")</code></pre>
            </section>

            <!-- Ipuclari -->
            <section>
                <h2>Performans Ipuclari</h2>

                <div class="card-grid">
                    <div class="card">
                        <h3>Bellek Tasarrufu</h3>
                        <ul>
                            <li><code>load_in_4bit=True</code> kullanin</li>
                            <li>Batch size'i dusurun</li>
                            <li><code>gradient_checkpointing="unsloth"</code></li>
                        </ul>
                    </div>

                    <div class="card">
                        <h3>Hiz Arttirma</h3>
                        <ul>
                            <li><code>packing=True</code> deneyin</li>
                            <li>BF16 kullanin (destekleniyorsa)</li>
                            <li><code>optim="adamw_8bit"</code></li>
                        </ul>
                    </div>

                    <div class="card">
                        <h3>Kalite Iyilestirme</h3>
                        <ul>
                            <li>Daha fazla epoch (2-3)</li>
                            <li>Daha yuksek LoRA rank (32-64)</li>
                            <li>Daha cok veri (1000+)</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- Hatalar -->
            <section>
                <h2>Sik Karsilasilan Hatalar</h2>

                <div class="card">
                    <h3>CUDA Out of Memory</h3>
                    <pre><code># Cozumler:
# 1. Batch size dusurun
per_device_train_batch_size=1

# 2. Gradient accumulation artirin
gradient_accumulation_steps=8

# 3. Sequence length dusurun
max_seq_length=1024</code></pre>
                </div>

                <div class="card">
                    <h3>Windows Triton Hatasi</h3>
                    <pre><code># SFTConfig'e ekleyin:
dataset_num_proc=1</code></pre>
                </div>
            </section>
        </div>
    </main>

    <footer>
        <div class="container">
            <p>
                Kaynaklar:
                <a href="https://docs.unsloth.ai/get-started/fine-tuning-llms-guide" target="_blank">Unsloth Fine-Tuning Guide</a> |
                <a href="https://huggingface.co/docs/trl" target="_blank">TRL Docs</a>
            </p>
        </div>
    </footer>
</body>
</html>
